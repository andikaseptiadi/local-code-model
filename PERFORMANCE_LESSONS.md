# Performance Optimization Lessons: Matrix Multiplication on ARM

## Overview

This document captures key lessons learned from implementing and benchmarking matrix multiplication across different optimization levels on ARM hardware. These insights come from real measurements on NVIDIA Jetson Orin NX (Cortex-A78AE + integrated GPU).

## The Surprising Results

### Benchmark: 1024√ó1024 FP64 Matrix Multiplication

| Implementation | GFLOPS | Speedup | Verdict |
|---------------|--------|---------|---------|
| Naive (single-thread) | 1.25 | 1√ó | Baseline |
| **NEON SIMD** | **0.93** | **0.74√ó** | ‚ö†Ô∏è **SLOWER!** |
| Parallel (8 cores) | ~8-10 | ~8√ó | Good scaling |
| **OpenBLAS** | **92** | **73√ó** | üèÜ **Winner** |
| CUDA GPU | 23 | 18√ó | Good but not best |

**Shocking finding**: Naive SIMD vectorization made performance **25% worse**.

## Lesson 1: SIMD Isn't Magic

### What We Expected
"NEON processes 2 float64 at once, so it should be 2√ó faster!"

### What Actually Happened
NEON was 25% **slower** than naive scalar code.

### Why It Failed

#### 1. Memory Access Patterns Matter More Than Vectorization

**The naive implementation**:
```c
// Simple, predictable memory access
for (i = 0; i < m; i++)
    for (j = 0; j < n; j++)
        for (k = 0; k < K; k++)
            C[i][j] += A[i][k] * B[k][j];  // Sequential reads
```

**The broken NEON implementation**:
```c
// Non-contiguous memory access!
for (int64_t vl_idx = 0; vl_idx < 2; vl_idx++) {
    b_vals[0] = b[(l + 0) * n + j];  // Stride = n
    b_vals[1] = b[(l + 1) * n + j];  // Stride = n (again)
}
float64x2_t b_vec = vld1q_f64(b_vals);  // Extra copy
```

**Problem**: Matrix B is stored in row-major order, but we're accessing it column-wise. With stride `n=1024`, every access is 8KB apart (1024 elements √ó 8 bytes). This **destroys cache locality**.

#### 2. The Cost of Data Marshaling

NEON code had to:
1. Extract values from strided memory ‚Üí temporary array
2. Load temporary array into NEON register
3. Perform NEON operations
4. Store results back

This overhead exceeded the 2√ó speedup from vectorization!

#### 3. Modern CPUs Are Smart

The naive code:
- Has predictable access patterns ‚Üí **hardware prefetcher works**
- Fits in L1 cache (if tiled) ‚Üí **cache-friendly**
- Compiler can optimize ‚Üí **auto-vectorization possible**

The NEON code:
- Strided access ‚Üí **prefetcher confused**
- Manual loads/stores ‚Üí **compiler can't optimize**
- Extra marshaling ‚Üí **actual overhead**

### The Fix: Memory Layout + Tiling

**Option 1: Transpose B** (best for this case)
```c
// Transpose B once: O(n¬≤)
for (i = 0; i < k; i++)
    for (j = 0; j < n; j++)
        B_T[j][i] = B[i][j];

// Now both A and B_T have sequential access
for (i = 0; i < m; i++)
    for (j = 0; j < n; j++) {
        // Load 2 elements from A (sequential)
        float64x2_t a_vec = vld1q_f64(&A[i][k]);
        // Load 2 elements from B_T (sequential!)
        float64x2_t b_vec = vld1q_f64(&B_T[j][k]);
        // Now NEON actually helps
    }
```

**Option 2: Cache Blocking** (what OpenBLAS does)
```c
#define BLOCK 64
for (ii = 0; ii < m; ii += BLOCK)
    for (jj = 0; jj < n; jj += BLOCK)
        for (kk = 0; kk < k; kk += BLOCK)
            // Process BLOCK√óBLOCK tile
            // Everything fits in L1 cache
```

## Lesson 2: Libraries Represent Decades of Work

### Why OpenBLAS Won (92 GFLOPS)

OpenBLAS achieved **73√ó speedup** over naive. Here's what it does that we didn't:

#### 1. Multi-Level Cache Blocking
```
L1 cache: 32-64 KB per core
L2 cache: 256-512 KB per core
L3 cache: 4-8 MB shared

OpenBLAS uses:
- L1 blocks: 16√ó16 or 32√ó32 (fits in 32KB)
- L2 blocks: 128√ó128 (fits in 512KB)
- L3 blocks: 1024√ó1024 (fits in 4MB)
```

Each level of blocking reduces cache misses.

#### 2. Register Blocking (Micro-Kernel)

OpenBLAS's inner loop processes 4√ó4 or 8√ó8 blocks using **all available registers**:

```asm
; Pseudo-assembly of OpenBLAS inner kernel
; Keeps 16 results in registers (4√ó4 block)
vld1.64 {q0-q1}, [A]   ; Load 4 from A
vld1.64 {q2-q3}, [B]   ; Load 4 from B
vfma.f64 q4, q0, q2    ; C[0,0-3] += A[0] * B[0-3]
vfma.f64 q5, q0, q3    ; C[0,4-7] += A[0] * B[4-7]
; ... 12 more operations
; No memory access in inner loop!
```

#### 3. Prefetching
```c
__builtin_prefetch(&A[i + 8][k]);  // Fetch ahead
__builtin_prefetch(&B[k][j + 8]);
```

Tells CPU to load data before it's needed.

#### 4. Architecture-Specific Tuning

OpenBLAS has **hand-written assembly** for Cortex-A78:
- Knows L1/L2/L3 sizes
- Knows NEON pipeline depth
- Knows memory bandwidth limits
- Uses optimal tile sizes

This is **10+ years of optimization work** we can't replicate in a day.

## Lesson 3: GPU Isn't Always Faster

### CUDA Got 23 GFLOPS - Why Not More?

#### 1. FP64 vs FP32 Performance

```
Orin GPU specifications:
- FP32: 1.5 TFLOPS (theoretical)
- FP64: 0.25 TFLOPS (1:64 ratio)
- FP16: 3.0 TFLOPS (2√ó FP32)
```

**We used FP64** because the teaching code uses `float64`. But GPUs are optimized for FP32/FP16!

**Achieved**: 23 GFLOPS = 9.2% of theoretical FP64 peak
**If we used FP32**: Would expect ~150-200 GFLOPS (10√ó better)

#### 2. Problem Size Matters

GPU performance vs matrix size:

| Size | GFLOPS | % of Peak | Notes |
|------|--------|-----------|-------|
| 64 | 1.5 | 0.6% | Overhead dominates |
| 256 | 12.6 | 5.0% | Getting better |
| 1024 | 22.4 | 9.0% | Reasonable |
| 4096 | 24.1 | 9.7% | Plateau |

**Lesson**: GPUs need **large** problems to be efficient. Below 512√ó512, CPU often wins.

#### 3. Data Transfer Overhead

```c
// Every GPU operation requires:
cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);  // ~1-5ms
cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);  // ~1-5ms
cublasDgemm(...);                                      // ~1ms for 1024√ó1024
cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);  // ~1-5ms

// Total: 10ms overhead + 1ms compute
// For small matrices, overhead > compute!
```

Even with unified memory (Jetson), there's still overhead.

#### 4. When GPU Wins

GPU would dominate if we:
- **Use FP32/FP16**: 64√ó more throughput
- **Use larger matrices**: 4096√ó4096 or bigger
- **Batch operations**: 100√ó 1024√ó1024 matrices
- **Keep data on GPU**: Avoid transfers

## Lesson 4: Measure, Don't Assume

### Assumptions We Had

1. ‚úó "SIMD is always faster than scalar" ‚Üí NEON was slower
2. ‚úó "GPU is always faster than CPU" ‚Üí OpenBLAS won
3. ‚úó "More cores = more speedup" ‚Üí Depends on memory bandwidth
4. ‚úì "Optimized libraries are hard to beat" ‚Üí OpenBLAS proved this

### The Importance of Profiling

```bash
# What we thought bottleneck was:
"Compute is too slow, need SIMD/GPU"

# What profiling showed:
"Memory access pattern causing 80% cache misses"
```

**Tools we should use**:
- `perf stat` - cache miss rates
- `perf record` - hotspot analysis
- `cachegrind` - cache simulation
- GPU profiler - kernel occupancy

## Lesson 5: The Optimization Hierarchy

Based on our results, here's the actual priority order:

### 1. Algorithm Complexity (Most Important)
```
Naive: O(n¬≥)
Strassen: O(n^2.807)
Coppersmith-Winograd: O(n^2.376)

10√ó gain possible from algorithm alone!
```

### 2. Memory Access Patterns
```
Sequential access: ~50 GB/s (DDR4)
Random access: ~5 GB/s (10√ó slower!)
Cache-blocked: ~200 GB/s (L1 cache)

40√ó gain from cache blocking!
```

### 3. Data Layout
```
Structure of Arrays (SoA): Cache-friendly
Array of Structures (AoS): Cache-unfriendly

2-5√ó gain from correct layout!
```

### 4. Parallelization
```
8 cores: 8√ó theoretical, 6√ó practical
Amdahl's law limits scaling

6-8√ó gain from threading!
```

### 5. Vectorization (Least Important!)
```
NEON: 2√ó for float64, 4√ó for float32
Only if memory is already optimal!

2-4√ó gain (if done right)!
```

**Our NEON implementation**: Did #5 without #2 and #3, so it got **slower**.

## Practical Recommendations

### For Learning (This Project)

1. ‚úÖ **Start with naive implementation**
   - Understand the algorithm
   - Establish baseline

2. ‚úÖ **Add cache blocking next**
   - Most important optimization
   - 10-20√ó speedup possible
   - Teaches memory hierarchy

3. ‚úÖ **Then add parallelization**
   - 6-8√ó speedup on 8 cores
   - Teaches concurrency

4. ‚è≥ **Then optimize memory layout**
   - Transpose matrices
   - Use better data structures

5. ‚è≥ **Finally add SIMD (if it helps)**
   - Only after memory is optimal
   - Benchmark each change!

6. ‚úÖ **Compare with libraries**
   - Shows years of optimization work
   - Teaches humility üòÑ

### For Production

1. **Use optimized libraries**:
   - OpenBLAS (CPU)
   - ARM Compute Library (ARM-specific)
   - cuBLAS (NVIDIA GPU)
   - ROCm BLAS (AMD GPU)

2. **Choose right precision**:
   - ML training: FP32 or mixed precision
   - ML inference: FP16 or INT8
   - Scientific computing: FP64 only if needed

3. **Choose right hardware**:
   - Small matrices (<1024): CPU + OpenBLAS
   - Large matrices (>2048): GPU (FP32/FP16)
   - Batch operations: GPU
   - FP64 scientific: High-end CPU or data center GPU

## Key Insights Summary

1. **Memory > Compute**: Bad memory access is worse than no vectorization
2. **Libraries > Manual**: OpenBLAS (73√ó) beat everything else
3. **FP64 on GPU is slow**: 1:64 ratio vs FP32
4. **Problem size matters**: GPU needs >1024 to be efficient
5. **Cache blocking is critical**: 10-20√ó gain before any SIMD
6. **Modern CPUs are smart**: Naive code with good access patterns beats broken SIMD
7. **Always profile**: Assumptions are often wrong
8. **Start simple**: Optimize where measurements show bottlenecks

## What's Next

To properly fix our implementations:

1. ‚è≥ **Fix NEON**: Add cache blocking, transpose B, improve memory access
2. ‚è≥ **Add ARM Compute Library**: See properly-optimized NEON
3. ‚è≥ **Test FP32 on GPU**: Show 64√ó performance improvement
4. ‚è≥ **Document with examples**: Teaching progression from naive ‚Üí optimal

## References

- [What Every Programmer Should Know About Memory](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf)
- [OpenBLAS: An Optimized BLAS Library](https://www.openblas.net/)
- [NVIDIA CUDA GEMM Optimization](https://docs.nvidia.com/cuda/cublas/index.html#cublas-GemmEx)
- [Gallery of Processor Cache Effects](https://igoro.com/archive/gallery-of-processor-cache-effects/)
- [ARM NEON Optimization](https://developer.arm.com/documentation/102467/latest/)
