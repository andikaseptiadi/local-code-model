{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Training a Transformer from Scratch\n",
    "\n",
    "In this notebook, we'll build and train a complete GPT-style transformer model. You'll see:\n",
    "- Complete transformer architecture assembly\n",
    "- Training loop with backpropagation\n",
    "- Adam optimizer in action\n",
    "- Loss curves and learning dynamics\n",
    "- Text generation from your trained model\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete notebooks 01 (tensors) and 02 (attention) first. This builds on those concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import (\n",
    "    \"fmt\"\n",
    "    \"math\"\n",
    "    \"math/rand\"\n",
    "    \"github.com/scttfrdmn/local-code-model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Training Data\n",
    "\n",
    "For this tutorial, we'll train on a tiny text corpus. In practice, you'd use much more data.\n",
    "\n",
    "**What's happening:**\n",
    "- Tokenize text into integer IDs\n",
    "- Create training sequences (input + target pairs)\n",
    "- Batch sequences for efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Simple training corpus\n",
    "corpus := []string{\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "}\n",
    "\n",
    "// Build character-level tokenizer\n",
    "tokenizer := main.NewTokenizer()\n",
    "tokenizer.SetType(\"char\")\n",
    "if err := tokenizer.Train(corpus, 100); err != nil {\n",
    "    panic(err)\n",
    "}\n",
    "\n",
    "fmt.Printf(\"Vocabulary size: %d\\n\", tokenizer.VocabSize())\n",
    "\n",
    "// Encode corpus\n",
    "var tokens []int\n",
    "for _, text := range corpus {\n",
    "    tokens = append(tokens, tokenizer.Encode(text)...)\n",
    "}\n",
    "\n",
    "fmt.Printf(\"Total tokens: %d\\n\", len(tokens))\n",
    "fmt.Printf(\"Sample tokens: %v\\n\", tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Training Dataset\n",
    "\n",
    "Split tokens into sequences for training. Each sequence has:\n",
    "- **Input**: tokens[i:i+seqLen]\n",
    "- **Target**: tokens[i+1:i+seqLen+1]\n",
    "\n",
    "The model learns to predict the next token given previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLen := 16\n",
    "batchSize := 2\n",
    "\n",
    "// Create dataset\n",
    "dataset := main.NewTextDataset(tokens, seqLen)\n",
    "\n",
    "fmt.Printf(\"Dataset size: %d sequences\\n\", dataset.Size())\n",
    "fmt.Printf(\"Number of batches: %d\\n\", dataset.Size()/batchSize)\n",
    "\n",
    "// Look at one example\n",
    "input, target := dataset.GetBatch(0, 1, seqLen)\n",
    "fmt.Printf(\"\\nExample training pair:\\n\")\n",
    "fmt.Printf(\"Input:  %v\\n\", input.Data()[:seqLen])\n",
    "fmt.Printf(\"Target: %v\\n\", target.Data()[:seqLen])\n",
    "fmt.Printf(\"Decoded input: %q\\n\", tokenizer.Decode(input.AsInts()[:seqLen]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the Transformer\n",
    "\n",
    "Now let's create a tiny GPT model:\n",
    "- 2 layers (very small!)\n",
    "- 32 embedding dimensions\n",
    "- 2 attention heads\n",
    "- 16 token context window\n",
    "\n",
    "This is ~10K parameters vs 125M for GPT-2 small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Model hyperparameters\n",
    "config := main.TransformerConfig{\n",
    "    VocabSize:    tokenizer.VocabSize(),\n",
    "    NumLayers:    2,\n",
    "    EmbedDim:     32,\n",
    "    NumHeads:     2,\n",
    "    FFDim:        64,  // Usually 4x embed_dim\n",
    "    SeqLen:       seqLen,\n",
    "    DropoutRate:  0.1,\n",
    "}\n",
    "\n",
    "model := main.NewTransformer(config)\n",
    "\n",
    "fmt.Printf(\"Model architecture:\\n\")\n",
    "fmt.Printf(\"  Layers: %d\\n\", config.NumLayers)\n",
    "fmt.Printf(\"  Embed dim: %d\\n\", config.EmbedDim)\n",
    "fmt.Printf(\"  Num heads: %d\\n\", config.NumHeads)\n",
    "fmt.Printf(\"  FF dim: %d\\n\", config.FFDim)\n",
    "fmt.Printf(\"  Sequence length: %d\\n\", config.SeqLen)\n",
    "\n",
    "// Count parameters\n",
    "numParams := model.NumParameters()\n",
    "fmt.Printf(\"\\nTotal parameters: %d\\n\", numParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Optimizer\n",
    "\n",
    "We'll use **Adam** (Adaptive Moment Estimation):\n",
    "- Maintains moving averages of gradients (momentum)\n",
    "- Adapts learning rate per parameter\n",
    "- Works much better than plain SGD\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Learning rate: 0.001 (will decay during training)\n",
    "- Beta1: 0.9 (momentum term)\n",
    "- Beta2: 0.999 (variance term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Adam optimizer\n",
    "learningRate := 0.001\n",
    "optimizer := main.NewAdamOptimizer(learningRate, 0.9, 0.999, 1e-8)\n",
    "\n",
    "fmt.Printf(\"Optimizer: Adam\\n\")\n",
    "fmt.Printf(\"  Initial learning rate: %.4f\\n\", learningRate)\n",
    "fmt.Printf(\"  Beta1 (momentum): %.2f\\n\", 0.9)\n",
    "fmt.Printf(\"  Beta2 (variance): %.3f\\n\", 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "The training loop:\n",
    "1. **Forward pass**: Compute model predictions\n",
    "2. **Loss**: Compare predictions to targets (cross-entropy)\n",
    "3. **Backward pass**: Compute gradients via backpropagation\n",
    "4. **Update**: Apply gradients using optimizer\n",
    "5. **Repeat**: Until loss converges\n",
    "\n",
    "We'll train for just 10 epochs on our tiny dataset to see the model learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs := 10\n",
    "numBatches := dataset.Size() / batchSize\n",
    "\n",
    "fmt.Printf(\"Training for %d epochs...\\n\\n\", epochs)\n",
    "\n",
    "// Track loss history\n",
    "var lossHistory []float64\n",
    "\n",
    "for epoch := 0; epoch < epochs; epoch++ {\n",
    "    epochLoss := 0.0\n",
    "    \n",
    "    for batch := 0; batch < numBatches; batch++ {\n",
    "        // Get batch\n",
    "        input, target := dataset.GetBatch(batch*batchSize, batchSize, seqLen)\n",
    "        \n",
    "        // Forward pass\n",
    "        logits := model.Forward(input, true)  // training=true for dropout\n",
    "        \n",
    "        // Compute loss\n",
    "        loss := main.CrossEntropyLoss(logits, target)\n",
    "        epochLoss += loss\n",
    "        \n",
    "        // Backward pass\n",
    "        gradLoss := main.Ones(logits.Shape()...)  // dloss/dlogits = 1 initially\n",
    "        model.Backward(gradLoss)\n",
    "        \n",
    "        // Update weights\n",
    "        optimizer.Step(model.Parameters())\n",
    "        \n",
    "        // Zero gradients\n",
    "        model.ZeroGrad()\n",
    "    }\n",
    "    \n",
    "    avgLoss := epochLoss / float64(numBatches)\n",
    "    lossHistory = append(lossHistory, avgLoss)\n",
    "    \n",
    "    fmt.Printf(\"Epoch %2d/%d, Loss: %.4f\\n\", epoch+1, epochs, avgLoss)\n",
    "}\n",
    "\n",
    "fmt.Printf(\"\\nTraining complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Training\n",
    "\n",
    "Let's plot the loss curve to see how the model learned over time.\n",
    "\n",
    "**What to look for:**\n",
    "- Loss should decrease (model is learning)\n",
    "- May fluctuate (small dataset, small batch size)\n",
    "- Eventual plateau (model capacity reached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt.Printf(\"Loss curve:\\n\")\n",
    "fmt.Printf(\"Epoch  Loss\\n\")\n",
    "fmt.Printf(\"-----  ----\\n\")\n",
    "\n",
    "for i, loss := range lossHistory {\n",
    "    // Simple ASCII bar chart\n",
    "    barLen := int(loss * 10)  // Scale for visibility\n",
    "    if barLen > 50 {\n",
    "        barLen = 50\n",
    "    }\n",
    "    bar := \"\"\n",
    "    for j := 0; j < barLen; j++ {\n",
    "        bar += \"█\"\n",
    "    }\n",
    "    fmt.Printf(\"%5d  %.4f %s\\n\", i+1, loss, bar)\n",
    "}\n",
    "\n",
    "// Summary statistics\n",
    "initialLoss := lossHistory[0]\n",
    "finalLoss := lossHistory[len(lossHistory)-1]\n",
    "improvement := initialLoss - finalLoss\n",
    "percentImprovement := (improvement / initialLoss) * 100\n",
    "\n",
    "fmt.Printf(\"\\nInitial loss: %.4f\\n\", initialLoss)\n",
    "fmt.Printf(\"Final loss:   %.4f\\n\", finalLoss)\n",
    "fmt.Printf(\"Improvement:  %.4f (%.1f%%)\\n\", improvement, percentImprovement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Text\n",
    "\n",
    "Now let's use our trained model to generate text!\n",
    "\n",
    "**How generation works:**\n",
    "1. Start with a prompt (seed text)\n",
    "2. Model predicts next token probability distribution\n",
    "3. Sample from distribution (with temperature)\n",
    "4. Append sampled token to input\n",
    "5. Repeat\n",
    "\n",
    "**Temperature** controls randomness:\n",
    "- Low (0.5): Conservative, predictable\n",
    "- Medium (1.0): Balanced\n",
    "- High (1.5): Creative, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Generation function\n",
    "func generate(model *main.Transformer, tokenizer *main.Tokenizer, prompt string, maxLen int, temperature float64) string {\n",
    "    // Encode prompt\n",
    "    tokens := tokenizer.Encode(prompt)\n",
    "    \n",
    "    // Generate tokens one by one\n",
    "    for i := 0; i < maxLen; i++ {\n",
    "        // Take last seqLen tokens as input\n",
    "        start := 0\n",
    "        if len(tokens) > seqLen {\n",
    "            start = len(tokens) - seqLen\n",
    "        }\n",
    "        input := tokens[start:]\n",
    "        \n",
    "        // Convert to tensor\n",
    "        inputTensor := main.NewTensor([]int{1, len(input)}, toFloat64Slice(input))\n",
    "        \n",
    "        // Forward pass (no dropout during inference)\n",
    "        logits := model.Forward(inputTensor, false)\n",
    "        \n",
    "        // Get logits for last position\n",
    "        lastLogits := logits.Slice(0, 1).Slice(len(input)-1, len(input))\n",
    "        \n",
    "        // Apply temperature\n",
    "        scaledLogits := lastLogits.Scale(1.0 / temperature)\n",
    "        \n",
    "        // Softmax to get probabilities\n",
    "        probs := scaledLogits.Softmax(1)\n",
    "        \n",
    "        // Sample next token\n",
    "        nextToken := sample(probs)\n",
    "        tokens = append(tokens, nextToken)\n",
    "        \n",
    "        // Stop if we hit end-of-sequence\n",
    "        if nextToken == tokenizer.EosID() {\n",
    "            break\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return tokenizer.Decode(tokens)\n",
    "}\n",
    "\n",
    "func toFloat64Slice(ints []int) []float64 {\n",
    "    floats := make([]float64, len(ints))\n",
    "    for i, v := range ints {\n",
    "        floats[i] = float64(v)\n",
    "    }\n",
    "    return floats\n",
    "}\n",
    "\n",
    "func sample(probs *main.Tensor) int {\n",
    "    // Sample from probability distribution\n",
    "    r := rand.Float64()\n",
    "    cumProb := 0.0\n",
    "    probData := probs.Data()\n",
    "    \n",
    "    for i, p := range probData {\n",
    "        cumProb += p\n",
    "        if r < cumProb {\n",
    "            return i\n",
    "        }\n",
    "    }\n",
    "    return len(probData) - 1  // Fallback\n",
    "}\n",
    "\n",
    "fmt.Println(\"Generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Some Text\n",
    "\n",
    "Let's try a few different prompts and temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Try different prompts\n",
    "prompts := []string{\n",
    "    \"The \",\n",
    "    \"To be\",\n",
    "    \"All that\",\n",
    "}\n",
    "\n",
    "temperature := 0.8\n",
    "maxLen := 30\n",
    "\n",
    "fmt.Printf(\"Generating text (temp=%.1f):\\n\\n\", temperature)\n",
    "\n",
    "for _, prompt := range prompts {\n",
    "    generated := generate(model, tokenizer, prompt, maxLen, temperature)\n",
    "    fmt.Printf(\"Prompt: %q\\n\", prompt)\n",
    "    fmt.Printf(\"Generated: %q\\n\\n\", generated)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Effect\n",
    "\n",
    "Compare different temperature settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt := \"The quick\"\n",
    "temperatures := []float64{0.3, 0.8, 1.5}\n",
    "\n",
    "fmt.Printf(\"Prompt: %q\\n\\n\", prompt)\n",
    "\n",
    "for _, temp := range temperatures {\n",
    "    generated := generate(model, tokenizer, prompt, 20, temp)\n",
    "    fmt.Printf(\"Temperature %.1f: %q\\n\", temp, generated)\n",
    "}\n",
    "\n",
    "fmt.Printf(\"\\nNotice how higher temperature produces more varied (and less coherent) text!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations! You've trained a transformer from scratch. Here's what we covered:\n",
    "\n",
    "1. **Data preparation**: Tokenization and sequence creation\n",
    "2. **Model architecture**: Complete transformer with attention and feed-forward layers\n",
    "3. **Training loop**: Forward pass → loss → backward pass → update\n",
    "4. **Optimization**: Adam optimizer with adaptive learning rates\n",
    "5. **Generation**: Autoregressive sampling with temperature control\n",
    "\n",
    "## What Makes This Work?\n",
    "\n",
    "- **Attention**: Model learns which tokens to focus on\n",
    "- **Self-supervision**: Next-token prediction provides unlimited training signal\n",
    "- **Depth**: Multiple layers compose increasingly abstract representations\n",
    "- **Scale**: More data + bigger models = better results\n",
    "\n",
    "## Limitations of This Tiny Model\n",
    "\n",
    "- **Tiny dataset**: Only a few sentences (real models: billions of tokens)\n",
    "- **Small model**: ~10K parameters (GPT-3: 175B parameters)\n",
    "- **Short context**: 16 tokens (modern models: 8K-100K tokens)\n",
    "- **No regularization**: Would overfit on real tasks\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**To improve this model:**\n",
    "1. Train on more data (download WikiText or similar)\n",
    "2. Increase model size (more layers, larger embeddings)\n",
    "3. Longer training (hundreds of epochs)\n",
    "4. Add learning rate scheduling (warmup + decay)\n",
    "5. Implement gradient clipping\n",
    "6. Use BPE tokenization instead of character-level\n",
    "\n",
    "**To learn more:**\n",
    "- Read the main codebase (see `../transformer.go`, `../train.go`)\n",
    "- Study attention patterns with `cmd_visualize.go`\n",
    "- Review backpropagation in `../docs/backpropagation.md`\n",
    "- Explore training dynamics in `../docs/training-dynamics.md`\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try modifying the hyperparameters:\n",
    "- Change `NumLayers` to 4\n",
    "- Increase `EmbedDim` to 64\n",
    "- Train for 20 epochs instead of 10\n",
    "\n",
    "Does the model learn better? Does generation improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Experiment with hyperparameters\n",
    "// Try:\n",
    "// - Different learning rates\n",
    "// - More layers\n",
    "// - Larger embeddings\n",
    "// - Longer training\n",
    "//\n",
    "// Observe how each change affects:\n",
    "// - Training loss curve\n",
    "// - Generation quality\n",
    "// - Training time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go",
   "language": "go",
   "name": "gophernotes"
  },
  "language_info": {
   "codemirror_mode": "",
   "file_extension": ".go",
   "mimetype": "",
   "name": "go",
   "nbconvert_exporter": "",
   "pygments_lexer": "",
   "version": "go1.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
