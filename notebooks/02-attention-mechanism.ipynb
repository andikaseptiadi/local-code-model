{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Building the Attention Mechanism from Scratch\n",
    "\n",
    "In this notebook, we'll build the core innovation of transformers: the **attention mechanism**.\n",
    "\n",
    "## What is Attention?\n",
    "\n",
    "Attention allows the model to focus on relevant parts of the input when processing each token. Think of reading a sentence - when you process the word \"it\", you look back to find what \"it\" refers to. That's attention!\n",
    "\n",
    "**Key Idea**: For each position, compute how much to \"attend\" to every other position.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete notebook 01 (tensor basics) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import (\n",
    "    \"fmt\"\n",
    "    \"math\"\n",
    "    \"github.com/scttfrdmn/local-code-model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Query, Key, Value (QKV)\n",
    "\n",
    "Attention has three components:\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I contain?\"\n",
    "- **Value (V)**: \"What information do I have?\"\n",
    "\n",
    "We project our input into these three spaces using learned weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Example: Convert input embeddings to Q, K, V\n",
    "batchSize := 2\n",
    "seqLen := 4\n",
    "embedDim := 8\n",
    "headDim := embedDim  // For single-head attention\n",
    "\n",
    "// Input: (batch, seq_len, embed_dim)\n",
    "x := main.RandN(batchSize, seqLen, embedDim)\n",
    "\n",
    "// Weight matrices for projections\n",
    "Wq := main.RandN(embedDim, headDim)\n",
    "Wk := main.RandN(embedDim, headDim)\n",
    "Wv := main.RandN(embedDim, headDim)\n",
    "\n",
    "// Project to Q, K, V\n",
    "// We need to reshape x to (batch*seq, embed) for matmul\n",
    "xFlat := x.Reshape(batchSize*seqLen, embedDim)\n",
    "Q := xFlat.MatMul(Wq).Reshape(batchSize, seqLen, headDim)\n",
    "K := xFlat.MatMul(Wk).Reshape(batchSize, seqLen, headDim)\n",
    "V := xFlat.MatMul(Wv).Reshape(batchSize, seqLen, headDim)\n",
    "\n",
    "fmt.Printf(\"Q shape: %v\\n\", Q.Shape())\n",
    "fmt.Printf(\"K shape: %v\\n\", K.Shape())\n",
    "fmt.Printf(\"V shape: %v\\n\", V.Shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute Attention Scores\n",
    "\n",
    "The attention score between position i and j is the dot product of Q[i] and K[j]:\n",
    "\n",
    "```\n",
    "scores[i,j] = Q[i] · K[j]ᵀ / √d\n",
    "```\n",
    "\n",
    "We divide by √d (d = head dimension) to prevent gradients from vanishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute attention scores: Q @ K^T\n",
    "// Q: (batch, seq, head_dim)\n",
    "// K: (batch, seq, head_dim)\n",
    "// Result: (batch, seq, seq)\n",
    "\n",
    "// For simplicity, let's work with first batch item\n",
    "Q0 := Q.Slice(0, 1).Reshape(seqLen, headDim)  // (seq, head_dim)\n",
    "K0 := K.Slice(0, 1).Reshape(seqLen, headDim)  // (seq, head_dim)\n",
    "V0 := V.Slice(0, 1).Reshape(seqLen, headDim)  // (seq, head_dim)\n",
    "\n",
    "// scores = Q @ K^T\n",
    "Kt := K0.Transpose(0, 1)  // (head_dim, seq)\n",
    "scores := Q0.MatMul(Kt)   // (seq, seq)\n",
    "\n",
    "// Scale by 1/sqrt(head_dim)\n",
    "scale := 1.0 / math.Sqrt(float64(headDim))\n",
    "scores = scores.Scale(scale)\n",
    "\n",
    "fmt.Printf(\"Attention scores shape: %v\\n\", scores.Shape())\n",
    "fmt.Printf(\"Sample scores (position 0 attending to all positions):\\n\")\n",
    "for j := 0; j < seqLen; j++ {\n",
    "    fmt.Printf(\"  [0→%d]: %.3f\\n\", j, scores.At(0, j))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply Softmax\n",
    "\n",
    "Convert scores to probabilities using softmax. Each row sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Apply softmax to each row\n",
    "attnWeights := main.Zeros(seqLen, seqLen)\n",
    "for i := 0; i < seqLen; i++ {\n",
    "    row := scores.Slice(i, i+1).Reshape(seqLen)\n",
    "    probs := row.Softmax(0)\n",
    "    for j := 0; j < seqLen; j++ {\n",
    "        attnWeights.Set(probs.At(j), i, j)\n",
    "    }\n",
    "}\n",
    "\n",
    "fmt.Printf(\"Attention weights (each row sums to 1):\\n\")\n",
    "for i := 0; i < seqLen; i++ {\n",
    "    fmt.Printf(\"Position %d attends to: \", i)\n",
    "    sum := 0.0\n",
    "    for j := 0; j < seqLen; j++ {\n",
    "        w := attnWeights.At(i, j)\n",
    "        fmt.Printf(\"%.3f \", w)\n",
    "        sum += w\n",
    "    }\n",
    "    fmt.Printf(\" (sum=%.3f)\\n\", sum)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Attention to Values\n",
    "\n",
    "Weighted sum of values using attention weights:\n",
    "\n",
    "```\n",
    "output[i] = Σⱼ attention_weights[i,j] * V[j]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_calls": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// output = attention_weights @ V\n",
    "output := attnWeights.MatMul(V0)  // (seq, head_dim)\n",
    "\n",
    "fmt.Printf(\"Output shape: %v\\n\", output.Shape())\n",
    "fmt.Printf(\"\\nInput (first position): %v\\n\", x.Slice(0, 1).Slice(0, 1).Data()[:4])\n",
    "fmt.Printf(\"Output (first position): %v\\n\", output.Slice(0, 1).Data()[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Instead of one attention head, use multiple heads in parallel. Each head learns different patterns.\n",
    "\n",
    "**Example**: One head might learn positional relationships, another learns semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Multi-head attention parameters\n",
    "numHeads := 2\n",
    "headDim = embedDim / numHeads  // Split embedding across heads\n",
    "\n",
    "fmt.Printf(\"Multi-head attention:\\n\")\n",
    "fmt.Printf(\"  Embed dim: %d\\n\", embedDim)\n",
    "fmt.Printf(\"  Num heads: %d\\n\", numHeads)\n",
    "fmt.Printf(\"  Head dim: %d\\n\", headDim)\n",
    "fmt.Printf(\"\\nEach head processes %d dimensions in parallel\\n\", headDim)\n",
    "fmt.Printf(\"Final output concatenates all heads: %d * %d = %d\\n\", \n",
    "    numHeads, headDim, numHeads*headDim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal (Masked) Attention\n",
    "\n",
    "For language modeling, we can't look at future tokens! Use a mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create causal mask (lower triangular)\n",
    "mask := main.Zeros(seqLen, seqLen)\n",
    "for i := 0; i < seqLen; i++ {\n",
    "    for j := 0; j <= i; j++ {\n",
    "        mask.Set(1.0, i, j)\n",
    "    }\n",
    "}\n",
    "\n",
    "fmt.Printf(\"Causal mask (1=can attend, 0=cannot):\\n\")\n",
    "for i := 0; i < seqLen; i++ {\n",
    "    for j := 0; j < seqLen; j++ {\n",
    "        fmt.Printf(\"%d \", int(mask.At(i, j)))\n",
    "    }\n",
    "    fmt.Println()\n",
    "}\n",
    "\n",
    "// Apply mask: set masked positions to -inf before softmax\n",
    "maskedScores := main.Zeros(seqLen, seqLen)\n",
    "for i := 0; i < seqLen; i++ {\n",
    "    for j := 0; j < seqLen; j++ {\n",
    "        if mask.At(i, j) > 0 {\n",
    "            maskedScores.Set(scores.At(i, j), i, j)\n",
    "        } else {\n",
    "            maskedScores.Set(-1e9, i, j)  // -inf\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fmt.Printf(\"\\nWith causal masking, position 1 can only attend to [0, 1]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Attention = weighted sum** of values based on query-key similarity\n",
    "2. **Three projections**: Q (query), K (key), V (value)\n",
    "3. **Scaled dot-product**: Divide by √d to stabilize gradients\n",
    "4. **Softmax**: Convert scores to probabilities\n",
    "5. **Multi-head**: Multiple attention patterns in parallel\n",
    "6. **Causal masking**: Prevent looking at future tokens\n",
    "\n",
    "## Complexity\n",
    "\n",
    "- **Time**: O(n² · d) where n = sequence length, d = dimension\n",
    "- **Space**: O(n²) for attention matrix\n",
    "\n",
    "This is why long sequences are expensive!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 3**: Build a complete transformer and train it\n",
    "- **Read**: `../docs/attention-mechanism.md` for deeper dive\n",
    "- **Explore**: Try different head dimensions and see how it affects learning\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Implement a simple attention function that takes Q, K, V and returns the attended output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Implement attention function\n",
    "// func attention(Q, K, V *Tensor, mask *Tensor) *Tensor {\n",
    "//     // 1. Compute scores: Q @ K^T\n",
    "//     // 2. Scale by 1/sqrt(d)\n",
    "//     // 3. Apply mask if provided\n",
    "//     // 4. Softmax\n",
    "//     // 5. Multiply by V\n",
    "//     return ???\n",
    "// }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go",
   "language": "go",
   "name": "gophernotes"
  },
  "language_info": {
   "name": "go",
   "version": "go1.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
